<#
.SYNOPSIS
    Main file processing orchestration for HashSmith - ENHANCED WITH CRITICAL FIXES

.DESCRIPTION
    This module orchestrates the main file processing workflow with parallel execution,
    progress tracking, error handling, and comprehensive result management.
    
    CRITICAL FIXES IMPLEMENTED:
    - Fixed resume logic bug (files were being reprocessed)
    - Dynamic thread management based on workload
    - Lock-free statistics with atomic operations
    - Smart chunking based on file size distribution
    - Graceful termination with CTRL+C handling
    - Enhanced logging with real-time progress
    - Performance optimizations for large datasets
#>

# Import required modules
# Note: Dependencies are handled by the main script import order

# Script-level variables for graceful termination
$Script:CancellationRequested = $false
$Script:ProcessingJob = $null

#region Enhanced Helper Functions

<#
.SYNOPSIS
    Calculates optimal chunk size based on file size distribution
#>
function Get-OptimalChunkSize {
    [CmdletBinding()]
    param(
        [System.IO.FileInfo[]]$Files,
        [int]$BaseChunkSize = 1000,
        [int]$MaxThreads = [Environment]::ProcessorCount
    )
    
    if ($Files.Count -eq 0) { return $BaseChunkSize }
    
    # Analyze file size distribution
    $smallFiles = @($Files | Where-Object { $_.Length -lt 1MB })
    $mediumFiles = @($Files | Where-Object { $_.Length -ge 1MB -and $_.Length -lt 100MB })
    $largeFiles = @($Files | Where-Object { $_.Length -ge 100MB })
    
    # Calculate memory footprint per chunk
    $avgFileSize = ($Files | Measure-Object -Property Length -Average).Average
    
    # Dynamic chunk sizing strategy
    if ($largeFiles.Count -gt ($Files.Count * 0.1)) {
        # Many large files - reduce chunk size to prevent memory issues
        $optimalSize = [Math]::Max(50, [Math]::Min($BaseChunkSize / 4, 250))
        Write-HashSmithLog -Message "Large file ratio detected: using reduced chunk size $optimalSize" -Level INFO -Component 'CHUNK'
    } elseif ($smallFiles.Count -gt ($Files.Count * 0.9)) {
        # Mostly small files - increase chunk size for better efficiency
        $optimalSize = [Math]::Min($BaseChunkSize * 2, 2000)
        Write-HashSmithLog -Message "Small file ratio detected: using increased chunk size $optimalSize" -Level INFO -Component 'CHUNK'
    } else {
        # Mixed files - use adaptive sizing
        $memoryPerFile = [Math]::Max(1KB, $avgFileSize / 1000)  # Estimate memory overhead
        $maxMemoryPerChunk = 100MB  # Conservative memory limit per chunk
        $optimalSize = [Math]::Max(100, [Math]::Min($BaseChunkSize, $maxMemoryPerChunk / $memoryPerFile))
        Write-HashSmithLog -Message "Mixed file sizes: using adaptive chunk size $optimalSize" -Level INFO -Component 'CHUNK'
    }
    
    return [int]$optimalSize
}

<#
.SYNOPSIS
    Calculates optimal thread count based on current workload
#>
function Get-OptimalThreadCount {
    [CmdletBinding()]
    param(
        [System.IO.FileInfo[]]$CurrentChunk,
        [int]$MaxThreads = [Environment]::ProcessorCount
    )
    
    if ($CurrentChunk.Count -eq 0) { return 1 }
    
    # Analyze current chunk characteristics
    $totalSize = ($CurrentChunk | Measure-Object -Property Length -Sum).Sum
    $avgFileSize = $totalSize / $CurrentChunk.Count
    $largeFileCount = @($CurrentChunk | Where-Object { $_.Length -gt 100MB }).Count
    
    # Dynamic thread calculation based on workload
    if ($largeFileCount -gt ($CurrentChunk.Count * 0.3)) {
        # Many large files - reduce threads to prevent I/O saturation
        $optimalThreads = [Math]::Max(2, [Math]::Min($MaxThreads / 2, 4))
        Write-HashSmithLog -Message "Large file workload: using $optimalThreads threads" -Level DEBUG -Component 'THREAD'
    } elseif ($avgFileSize -lt 1MB) {
        # Small files - use more threads for parallel I/O
        $optimalThreads = [Math]::Min($MaxThreads, [Math]::Max(4, $CurrentChunk.Count / 25))
        Write-HashSmithLog -Message "Small file workload: using $optimalThreads threads" -Level DEBUG -Component 'THREAD'
    } else {
        # Medium files - balanced approach
        $optimalThreads = [Math]::Round($MaxThreads * 0.8)
        if ($optimalThreads -lt 1) { $optimalThreads = 1 }
        Write-HashSmithLog -Message "Balanced workload: using $optimalThreads threads" -Level DEBUG -Component 'THREAD'
    }
    
    return [int]$optimalThreads
}

<#
.SYNOPSIS
    Registers graceful termination handler
#>
function Register-GracefulTermination {
    [CmdletBinding()]
    param(
        [string]$LogPath
    )
    
    # Register CTRL+C handler
    Register-EngineEvent -SourceIdentifier PowerShell.Exiting -Action {
        $Script:CancellationRequested = $true
        Write-Host "`nüõë Graceful shutdown initiated..." -ForegroundColor Yellow
        
        # Stop any running jobs
        if ($Script:ProcessingJob) {
            Stop-Job $Script:ProcessingJob -ErrorAction SilentlyContinue
            Remove-Job $Script:ProcessingJob -Force -ErrorAction SilentlyContinue
        }
        
        # Flush final logs
        try {
            Clear-HashSmithLogBatch -LogPath $using:LogPath
            Write-Host "üìù Final log batch flushed" -ForegroundColor Green
        } catch {
            Write-Warning "Failed to flush final logs: $($_.Exception.Message)"
        }
        
        Write-Host "‚úÖ Graceful shutdown complete" -ForegroundColor Green
    }
    
    # Also handle Console.CancelKeyPress for better integration
    [Console]::TreatControlCAsInput = $false
}

#endregion

#region Public Functions

<#
.SYNOPSIS
    Processes files with enhanced parallel execution, dynamic optimization, and critical bug fixes

.DESCRIPTION
    Orchestrates the main file processing workflow with:
    - FIXED: Resume logic now properly filters already processed files
    - Dynamic thread management based on workload characteristics
    - Lock-free statistics with atomic operations
    - Smart chunking based on file size distribution
    - Graceful termination with CTRL+C handling
    - Enhanced progress tracking with real-time updates
    - Performance optimizations for large datasets

.PARAMETER Files
    Array of FileInfo objects to process

.PARAMETER LogPath
    Path to the log file for results

.PARAMETER Algorithm
    Hash algorithm to use

.PARAMETER ExistingEntries
    Previously processed entries for resume operations

.PARAMETER BasePath
    Base path for creating relative paths

.PARAMETER StrictMode
    Enable strict mode with maximum validation

.PARAMETER VerifyIntegrity
    Enable integrity verification

.PARAMETER MaxThreads
    Maximum number of parallel threads

.PARAMETER ChunkSize
    Base number of files to process per chunk (will be optimized dynamically)

.PARAMETER RetryCount
    Number of retry attempts

.PARAMETER TimeoutSeconds
    Timeout for file operations

.PARAMETER ProgressTimeoutMinutes
    Timeout in minutes for no progress before stopping

.PARAMETER ShowProgress
    Show progress information

.PARAMETER UseParallel
    Use parallel processing (PowerShell 7+ only)

.EXAMPLE
    $hashes = Start-HashSmithFileProcessing -Files $files -LogPath $log -Algorithm "SHA256" -BasePath $base
#>
function Start-HashSmithFileProcessing {
    [CmdletBinding()]
    [OutputType([hashtable])]
    param(
        [Parameter(Mandatory)]
        [System.IO.FileInfo[]]$Files,
        
        [Parameter(Mandatory)]
        [string]$LogPath,
        
        [Parameter(Mandatory)]
        [ValidateSet('MD5', 'SHA1', 'SHA256', 'SHA512')]
        [string]$Algorithm,
        
        [hashtable]$ExistingEntries = @{},
        
        [string]$BasePath,
        
        [switch]$StrictMode,
        
        [switch]$VerifyIntegrity,
        
        [ValidateRange(1, 64)]
        [int]$MaxThreads = [Environment]::ProcessorCount,
        
        [ValidateRange(100, 5000)]
        [int]$ChunkSize = 1000,
        
        [ValidateRange(1, 10)]
        [int]$RetryCount = 3,
        
        [ValidateRange(10, 300)]
        [int]$TimeoutSeconds = 30,
        
        [ValidateRange(5, 1440)]
        [int]$ProgressTimeoutMinutes = 120,
        
        [switch]$ShowProgress,
        
        [switch]$UseParallel
    )
    
    Write-HashSmithLog -Message "Starting ENHANCED file processing with critical fixes" -Level INFO -Component 'PROCESS'
    Write-HashSmithLog -Message "Algorithm: $Algorithm, Files: $($Files.Count), Strict Mode: $StrictMode" -Level INFO -Component 'PROCESS'
    
    # Register graceful termination handler
    Register-GracefulTermination -LogPath $LogPath
    
    # CRITICAL FIX: Properly filter already processed files for resume functionality
    $filesToProcess = @()
    $skippedResumeCount = 0
    
    Write-Host "üîç Filtering files for resume operation..." -ForegroundColor Cyan
    
    foreach ($file in $Files) {
        # Create both absolute and relative path keys to check against existing entries
        $absolutePath = $file.FullName
        $relativePath = if ($BasePath) {
            $absolutePath.Substring($BasePath.Length).TrimStart('\', '/')
        } else {
            $absolutePath
        }
        
        # Check if file was already processed successfully (check both path formats)
        $alreadyProcessed = $ExistingEntries.Processed.ContainsKey($absolutePath) -or 
                           $ExistingEntries.Processed.ContainsKey($relativePath) -or
                           $ExistingEntries.Processed.ContainsKey($file.FullName)
        
        if ($alreadyProcessed) {
            $skippedResumeCount++
            if ($skippedResumeCount % 1000 -eq 0) {
                Write-Host "`r   ‚úÖ Skipped: $skippedResumeCount already processed" -NoNewline -ForegroundColor Green
            }
        } else {
            $filesToProcess += $file
        }
    }
    
    Write-Host "`r   ‚úÖ Resume filtering complete: $skippedResumeCount skipped, $($filesToProcess.Count) to process" -ForegroundColor Green
    Write-HashSmithLog -Message "RESUME: Skipped $skippedResumeCount already processed files, $($filesToProcess.Count) remaining" -Level SUCCESS -Component 'RESUME'
    
    if ($filesToProcess.Count -eq 0) {
        Write-Host "üéâ All files already processed - nothing to do!" -ForegroundColor Green
        return @{}
    }
    
    # Calculate optimal processing parameters
    $optimalChunkSize = Get-OptimalChunkSize -Files $filesToProcess -BaseChunkSize $ChunkSize -MaxThreads $MaxThreads
    Write-HashSmithLog -Message "Dynamic chunk sizing: $ChunkSize ‚Üí $optimalChunkSize" -Level INFO -Component 'OPTIMIZE'
    
    # Initialize enhanced processing variables with atomic operations
    $processedCount = [System.Threading.Interlocked]::Exchange([ref]$null, 0)
    $errorCount = [System.Threading.Interlocked]::Exchange([ref]$null, 0)
    $totalBytes = [long]0
    $fileHashes = [System.Collections.Concurrent.ConcurrentDictionary[string, object]]::new()
    $lastProgressUpdate = Get-Date
    
    # Enhanced chunking with dynamic optimization
    $totalChunks = [Math]::Ceiling($filesToProcess.Count / $optimalChunkSize)
    
    Write-Host "üöÄ Enhanced processing: $($filesToProcess.Count) files in $totalChunks optimized chunks" -ForegroundColor Cyan
    
    for ($i = 0; $i -lt $filesToProcess.Count -and -not $Script:CancellationRequested; $i += $optimalChunkSize) {
        $endIndex = [Math]::Min($i + $optimalChunkSize - 1, $filesToProcess.Count - 1)
        $chunk = $filesToProcess[$i..$endIndex]
        $chunkNumber = [Math]::Floor($i / $optimalChunkSize) + 1
        
        # Dynamic thread optimization per chunk
        $optimalThreads = Get-OptimalThreadCount -CurrentChunk $chunk -MaxThreads $MaxThreads
        
        # Professional chunk header with optimization info
        Write-Host ""
        Write-Host "‚ö° Processing Chunk $chunkNumber of $totalChunks (OPTIMIZED)" -ForegroundColor Cyan
        Write-Host "   Files: $($chunk.Count) | Threads: $optimalThreads | Range: $($i + 1) - $($endIndex + 1)" -ForegroundColor Gray
        
        $chunkStartTime = Get-Date
        
        # Check for cancellation
        if ($Script:CancellationRequested) {
            Write-Host "üõë Cancellation requested - stopping processing" -ForegroundColor Yellow
            break
        }
        
        # Enhanced parallel processing with optimized thread count
        if ($UseParallel -and $PSVersionTable.PSVersion.Major -ge 7) {
            # Initialize progress variables
            $totalChunkFiles = $chunk.Count
            $lastProgressCount = 0
            $lastProgressTime = Get-Date
            
            Write-Host "   üöÄ Parallel execution with $optimalThreads optimized threads" -ForegroundColor Green
            
            # Process files with dynamically optimized parallel jobs
            $chunkResults = $chunk | ForEach-Object -Parallel {
                # Import required variables into parallel runspace
                $Algorithm = $using:Algorithm
                $RetryCount = $using:RetryCount
                
                # Process single file with enhanced error handling
                $file = $_
                $result = @{
                    Path = $file.FullName
                    Size = $file.Length
                    Modified = $file.LastWriteTime
                    Success = $false
                    Hash = $null
                    Error = $null
                    ErrorCategory = 'Unknown'
                    Duration = 0
                    IsSymlink = $false
                    RaceConditionDetected = $false
                    IntegrityVerified = $false
                    Attempts = 1
                    FileName = $file.Name
                }
                
                $startTime = Get-Date
                
                try {
                    # Enhanced hash computation with proper resource management
                    $hashAlgorithm = [System.Security.Cryptography.HashAlgorithm]::Create($Algorithm)
                    try {
                        # Optimized file stream with better sharing options
                        $fileStream = [System.IO.File]::Open($file.FullName, 'Open', 'Read', 'ReadWrite')
                        try {
                            # Use streaming for large files
                            if ($file.Length -gt 100MB) {
                                $buffer = [byte[]]::new(64KB)
                                $totalRead = 0
                                
                                while ($totalRead -lt $file.Length) {
                                    $bytesToRead = [Math]::Min($buffer.Length, $file.Length - $totalRead)
                                    $bytesRead = $fileStream.Read($buffer, 0, $bytesToRead)
                                    if ($bytesRead -eq 0) { break }
                                    
                                    if ($totalRead + $bytesRead -eq $file.Length) {
                                        $hashAlgorithm.TransformFinalBlock($buffer, 0, $bytesRead) | Out-Null
                                    } else {
                                        $hashAlgorithm.TransformBlock($buffer, 0, $bytesRead, $null, 0) | Out-Null
                                    }
                                    $totalRead += $bytesRead
                                }
                                $hashBytes = $hashAlgorithm.Hash
                            } else {
                                # Standard computation for smaller files
                                $hashBytes = $hashAlgorithm.ComputeHash($fileStream)
                            }
                            
                            $result.Hash = [System.BitConverter]::ToString($hashBytes) -replace '-', ''
                            $result.Hash = $result.Hash.ToLower()
                            $result.Success = $true
                        }
                        finally {
                            if ($fileStream) { $fileStream.Dispose() }
                        }
                    }
                    finally {
                        if ($hashAlgorithm) { $hashAlgorithm.Dispose() }
                    }
                }
                catch {
                    $result.Error = $_.Exception.Message
                    $result.ErrorCategory = if ($_.Exception -is [System.IO.IOException]) { 'IO' } else { 'ProcessingError' }
                }
                
                $result.Duration = (Get-Date) - $startTime
                return $result
                
            } -ThrottleLimit $optimalThreads -AsJob
            
            # Store job reference for graceful termination
            $Script:ProcessingJob = $chunkResults
            
            # Enhanced progress monitoring with timeout handling
            $spinChars = @('‚†ã', '‚†ô', '‚†π', '‚†∏', '‚†º', '‚†¥', '‚†¶', '‚†ß', '‚†á', '‚†è')
            $spinIndex = 0
            
            while ($chunkResults.State -eq 'Running' -and -not $Script:CancellationRequested) {
                $char = $spinChars[$spinIndex % $spinChars.Length]
                $elapsed = (Get-Date) - $chunkStartTime
                $elapsedStr = if ($elapsed.TotalMinutes -gt 1) { 
                    "$([Math]::Floor($elapsed.TotalMinutes))m $([Math]::Floor($elapsed.Seconds))s" 
                } else { 
                    "$([Math]::Floor($elapsed.TotalSeconds))s" 
                }
                
                # Get current progress with error handling
                $currentResults = @()
                try {
                    $currentResults = @(Receive-Job $chunkResults -Keep -ErrorAction SilentlyContinue)
                } catch {
                    # Silently handle progress check errors
                }
                
                $completedCount = $currentResults.Count
                $progressPercent = if ($totalChunkFiles -gt 0) { 
                    [Math]::Round(($completedCount / $totalChunkFiles) * 100, 1) 
                } else { 0 }
                
                # Enhanced timeout with progress tracking
                $timeSinceLastProgress = (Get-Date) - $lastProgressTime
                if ($completedCount -gt $lastProgressCount) {
                    $lastProgressCount = $completedCount
                    $lastProgressTime = Get-Date
                } elseif ($timeSinceLastProgress.TotalMinutes -gt $ProgressTimeoutMinutes) {
                    Write-Host "`r   ‚ö†Ô∏è  No progress for $ProgressTimeoutMinutes minutes, stopping chunk..." -ForegroundColor Red
                    Stop-Job $chunkResults -ErrorAction SilentlyContinue
                    break
                }
                
                # Real-time progress display
                $progressMsg = "   $char Processing: $completedCount/$totalChunkFiles ($progressPercent%) | $elapsedStr"
                Write-Host "`r$progressMsg$(' ' * 10)" -NoNewline -ForegroundColor Yellow
                
                Start-Sleep -Milliseconds 150
                $spinIndex++
            }
            
            # Get final results with enhanced error handling
            try {
                if (-not $Script:CancellationRequested) {
                    $chunkResults = Receive-Job $chunkResults -Wait -ErrorAction Stop
                }
                Remove-Job $chunkResults -Force -ErrorAction SilentlyContinue
                $Script:ProcessingJob = $null
            }
            catch {
                Write-HashSmithLog -Message "Error retrieving parallel job results: $($_.Exception.Message)" -Level ERROR -Component 'PROCESS'
                $chunkResults = @()
            }
            
            Write-Host "`r$(' ' * 80)`r" -NoNewline
        } else {
            # Enhanced sequential processing with better progress
            Write-Host "   ‚öôÔ∏è  Sequential processing mode" -ForegroundColor Gray
            $chunkResults = @()
            $chunkFileCount = 0
            
            foreach ($file in $chunk) {
                if ($Script:CancellationRequested) { break }
                
                $chunkFileCount++
                
                $fileName = Split-Path $file.FullName -Leaf
                $progressPercent = [Math]::Round(($chunkFileCount / $chunk.Count) * 100, 1)
                
                Write-Host "`r   üîÑ Processing: $fileName ($progressPercent%)" -NoNewline -ForegroundColor Cyan
                
                $result = Get-HashSmithFileHashSafe -Path $file.FullName -Algorithm $Algorithm -RetryCount $RetryCount -TimeoutSeconds $TimeoutSeconds -VerifyIntegrity:$VerifyIntegrity -StrictMode:$StrictMode -PreIntegritySnapshot $file.IntegritySnapshot
                
                # Enhanced result mapping
                $result.Path = $file.FullName
                $result.Size = $file.Length
                $result.Modified = $file.LastWriteTime
                $result.IsSymlink = Test-HashSmithSymbolicLink -Path $file.FullName
                
                if (-not $result.ContainsKey('IntegrityVerified')) {
                    $result.IntegrityVerified = if ($result.ContainsKey('Integrity')) { [bool]$result.Integrity } else { $false }
                }
                
                $chunkResults += $result
                Start-Sleep -Milliseconds 5  # Reduced delay for better performance
            }
            
            Write-Host "`r$(' ' * 80)`r" -NoNewline
        }
        
        if ($Script:CancellationRequested) {
            Write-Host "üõë Processing stopped due to cancellation request" -ForegroundColor Yellow
            break
        }
        
        # Process results with enhanced atomic operations
        $chunkSuccessCount = 0
        $chunkErrorCount = 0
        
        foreach ($result in $chunkResults) {
            [System.Threading.Interlocked]::Increment([ref]$processedCount) | Out-Null
            
            if ($result.Success) {
                $chunkSuccessCount++
                
                # Thread-safe storage in concurrent dictionary
                $fileHashes.TryAdd($result.Path, @{
                    Hash = $result.Hash
                    Size = $result.Size
                    IsSymlink = $result.IsSymlink
                    RaceConditionDetected = $result.RaceConditionDetected
                    IntegrityVerified = $result.IntegrityVerified
                }) | Out-Null
                
                # Write to log with enhanced error handling
                try {
                    Write-HashSmithHashEntry -LogPath $LogPath -FilePath $result.Path -Hash $result.Hash -Size $result.Size -Modified $result.Modified -BasePath $BasePath -IsSymlink $result.IsSymlink -RaceConditionDetected $result.RaceConditionDetected -IntegrityVerified $result.IntegrityVerified -UseBatching
                }
                catch {
                    Write-HashSmithLog -Message "Failed to write log entry: $($_.Exception.Message)" -Level WARN -Component 'PROCESS'
                }
                
                # Atomic statistics updates
                [System.Threading.Interlocked]::Add([ref]$totalBytes, $result.Size) | Out-Null
                Add-HashSmithStatistic -Name 'FilesProcessed' -Amount 1
                Add-HashSmithStatistic -Name 'BytesProcessed' -Amount $result.Size
                
                if ($result.RaceConditionDetected) {
                    Add-HashSmithStatistic -Name 'FilesRaceCondition' -Amount 1
                }
            } else {
                $chunkErrorCount++
                [System.Threading.Interlocked]::Increment([ref]$errorCount) | Out-Null
                
                # Write error to log
                try {
                    Write-HashSmithHashEntry -LogPath $LogPath -FilePath $result.Path -Size $result.Size -Modified $result.Modified -ErrorMessage $result.Error -ErrorCategory $result.ErrorCategory -BasePath $BasePath -IsSymlink $result.IsSymlink -RaceConditionDetected $result.RaceConditionDetected -UseBatching
                }
                catch {
                    Write-HashSmithLog -Message "Failed to write error log entry: $($_.Exception.Message)" -Level WARN -Component 'PROCESS'
                }
                
                Add-HashSmithStatistic -Name 'FilesError' -Amount 1
                
                # Enhanced error categorization
                if ($result.ErrorCategory -in @('IO', 'Unknown')) {
                    Add-HashSmithStatistic -Name 'RetriableErrors' -Amount 1
                } else {
                    Add-HashSmithStatistic -Name 'NonRetriableErrors' -Amount 1
                }
            }
        }
        
        # Enhanced chunk completion summary
        $chunkElapsed = (Get-Date) - $chunkStartTime
        $filesPerSecond = if ($chunkElapsed.TotalSeconds -gt 0) { $chunk.Count / $chunkElapsed.TotalSeconds } else { 0 }
        
        Write-Host "   ‚úÖ Chunk completed: $chunkSuccessCount success, $chunkErrorCount errors" -ForegroundColor Green
        Write-Host "   ‚è±Ô∏è  Time: $($chunkElapsed.TotalSeconds.ToString('F1'))s | Rate: $($filesPerSecond.ToString('F1')) files/sec | Threads: $optimalThreads" -ForegroundColor Blue
        
        # Enhanced progress reporting
        if ($chunkNumber % 5 -eq 0 -or $chunkNumber -eq $totalChunks) {
            $overallPercent = [Math]::Round(($chunkNumber / $totalChunks) * 100, 1)
            $stats = Get-HashSmithStatistics
            $overallElapsed = if ($stats.StartTime) { (Get-Date) - $stats.StartTime } else { New-TimeSpan }
            
            $eta = if ($overallPercent -gt 5) {
                $totalEstimated = ($overallElapsed.TotalMinutes / $overallPercent) * 100
                $remaining = $totalEstimated - $overallElapsed.TotalMinutes
                if ($remaining -gt 60) { 
                    "$([Math]::Floor($remaining / 60))h $([Math]::Floor($remaining % 60))m" 
                } else { 
                    "$([Math]::Floor($remaining))m" 
                }
            } else { 
                "calculating..." 
            }
            
            Write-Host ""
            Write-Host "üìä Overall Progress: $overallPercent% | ETA: $eta | Errors: $errorCount" -ForegroundColor Magenta
        }
        
        # Flush log batch for data safety
        Clear-HashSmithLogBatch -LogPath $LogPath
        
        # Dynamic chunk size adjustment based on performance
        if ($chunkNumber -lt $totalChunks -and $filesPerSecond -gt 0) {
            $targetTimePerChunk = 30  # seconds
            if ($chunkElapsed.TotalSeconds -lt $targetTimePerChunk * 0.5) {
                $optimalChunkSize = [Math]::Min($optimalChunkSize * 1.2, 3000)
                Write-HashSmithLog -Message "Chunk too fast: increasing size to $optimalChunkSize" -Level DEBUG -Component 'OPTIMIZE'
            } elseif ($chunkElapsed.TotalSeconds -gt $targetTimePerChunk * 2) {
                $optimalChunkSize = [Math]::Max($optimalChunkSize * 0.8, 50)
                Write-HashSmithLog -Message "Chunk too slow: decreasing size to $optimalChunkSize" -Level DEBUG -Component 'OPTIMIZE'
            }
        }
        
        # Brief pause between chunks for system stability
        if ($chunkNumber -lt $totalChunks -and -not $Script:CancellationRequested) {
            Start-Sleep -Milliseconds 300
        }
        
        # Safety check for excessive errors
        if ($errorCount -gt ($filesToProcess.Count * 0.5) -and $filesToProcess.Count -gt 100) {
            Write-HashSmithLog -Message "Stopping processing due to high error rate: $errorCount errors" -Level ERROR -Component 'PROCESS'
            Set-HashSmithExitCode -ExitCode 3
            break
        }
    }
    
    # Final cleanup and log flush
    Clear-HashSmithLogBatch -LogPath $LogPath
    
    # Convert concurrent dictionary to regular hashtable for return
    $resultHashtable = @{}
    foreach ($kvp in $fileHashes.GetEnumerator()) {
        $resultHashtable[$kvp.Key] = $kvp.Value
    }
    
    if ($Script:CancellationRequested) {
        Write-Host ""
        Write-Host "üõë Processing terminated by user request" -ForegroundColor Yellow
        Set-HashSmithExitCode -ExitCode 130  # SIGINT exit code
    } else {
        Write-Host ""
        Write-Host "üéâ Enhanced file processing completed!" -ForegroundColor Green
    }
    
    $finalStats = Get-HashSmithStatistics
    Write-HashSmithLog -Message "ENHANCED processing completed: $($resultHashtable.Count) files processed successfully" -Level SUCCESS -Component 'PROCESS'
    Write-HashSmithLog -Message "Performance: $($finalStats.FilesProcessed) processed, $errorCount errors" -Level INFO -Component 'PROCESS'
    
    return $resultHashtable
}

#endregion

# Export public functions
Export-ModuleMember -Function @(
    'Start-HashSmithFileProcessing'
)